# -*- coding: utf-8 -*-
"""Customer_Chrun_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kkTFxc0Bgq_uh4Wza5nigwqJF5k6CrGQ

# RICHO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/progresproject/posting/Customer_Chrun_Prediction.csv')

df.head()

df.tail()

df.shape

df = df.drop(['RowNumber','CustomerId','Surname'], axis=1)

df.isnull().sum()

df.info()

df.duplicated().sum()

df.describe()

df.rename(columns={'Exited':'Churn'}, inplace=True)

"""# Explorative Data Analysis

## Pie Chart for Customer Churn
"""

plt.figure(figsize=(10,6))
plt.pie(df['Churn'].value_counts(),labels=['No','Yes'],autopct='%1.2f%%')
plt.title('Churn Percentage')
plt.show()

"""Diagram lingkaran dengan jelas memvisualisasikan churn pelanggan dalam kumpulan data. Mayoritas nasabah dalam kumpulan data terus menggunakan layanan bank (blue color), namun terdapat juga pemberhentian layanan dengan hanya 20,37% (orange color).

## Gender
"""

sns.countplot(x = 'Gender', data = df, hue = 'Churn')
plt.title('Gender Distribution')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

"""As seen in the graph, the majority of customers are men. However, if we look at customer churn, we can see that women have a greater tendency to churn than men. However, there is not much difference between the churn numbers of the two genders so we cannot make a hypothesis regarding customer churn based on customer gender.

## Age Distribution
"""

sns.histplot(data=df, x="Age", hue="Churn", multiple="stack",kde=True)

"""This histtogram visualizes the age distribution and the churn count of the customers. The majority of customers are from the age group 30-40 years old. However the customer churn count is highest for the customers of age 40 and 50. Therefore, age plays a significant role in customer churn, where late adults are more likely to churn as compared to young adults with minimal churn count.

## Credit Score
"""

fig, ax = plt.subplots(1,2,figsize=(15, 5))
sns.boxplot(x="Churn", y="CreditScore", data=df, ax=ax[0])
sns.violinplot(x="Churn", y="CreditScore", data=df, ax=ax[1])

"""In the boxplot, the median of both the churn and non churn customers are almost same. In addition to that, the shape of violinplot is also similar for both the churn and non churn customers. However some churn customers have low credit score, but on the whole, the credit score is not a good indicator of churn.

## Customer Location
"""

sns.countplot(x = 'Geography', hue = 'Churn', data = df)
plt.title('Geography and Churn')
plt.xlabel('Geography')
plt.ylabel('Count')
plt.show()

"""Majority of the customers are from France, followed by Spain and Germany. However in contrast to that France  has the highest number of customer curn followed by Germany and Spain. From this we can infer that France customers are more likely to churn than the customers from other countries.

## Tenure
"""

fig,ax = plt.subplots(1,2,figsize=(15,5))
sns.countplot(x='Tenure', data=df,ax=ax[0])
sns.countplot(x='Tenure', hue='Churn', data=df,ax=ax[1])

"""Looking at the churn of these customers based on their tenure, it can be observed that customers with tenure 1-9 years have higher churn count with maximum in customers with 1 year tenure followed those with 9 year tenure. However customers more than 9 years on tenure counts for the least churn. This is because the customers with higher tenure are more loyal to the bank and less likely to churn.

## Bank Balance
"""

sns.histplot(data=df, x="Balance", hue="Churn", multiple="stack",kde=True)

"""A huge number of customers have zero bank balance which also resulted in them leaving the bank. However, customer having bank balance between 100000 to 150000 are more likely to leave the bank after the customers with zero bank balance.

## Number of Products Purchased
"""

sns.countplot(x='NumOfProducts', hue='Churn', data=df)

"""In the dataset, we have customers in four categories according to the number of products purchased. The customers with purchase or 1 or 2 products are highest in number and have low churn count in comparison to the non churn customers in the category. However, in the category where customers have purchased 3 or 4 products the number of leaving customers is much higher than the non leaving customers. Therefore, the number of product purchased is a good indicator of customer churn.

## Customers with/without credit card
"""

sns.countplot(x=df['HasCrCard'],hue=df['Churn'])

"""Majoity of the customers have credit cars i.e. nealy 70% of the customers have credit cards leaving 30% of the customers who do not have credit cards. Moreover, the number of customers leaving the bank are more whom have a credit card.

## Active Members
"""

sns.countplot(x='IsActiveMember', hue='Churn', data=df)

"""As expected, the churn count is higher for non active members as compared to the active members of the bank. This is because the active members are more satisfied with the services of the bank and hence they are less likely to leave the bank. Therefore, the bank should focus on the non active members and try to improve their services to retain them.

## Estimated Salary
"""

sns.histplot(data=df,x='EstimatedSalary',hue='Churn',multiple='stack',palette='Set2')

"""This graph shows the distribution of the estimated salary of the customers along with the churn count. On the whole the there is no definite pattern in the salary distribution of the customers who churned and who didn't. Therefore estimated salary is not a good predictor of churn.

# DATA PREPROCESSING 2

## Label Encoding the variables
"""

variables = ['Geography','Gender']
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
for i in variables:
    le.fit(df[i].unique())
    df[i]=le.transform(df[i])
    print(i,df[i].unique())

"""## Normalization"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['CreditScore','Balance','EstimatedSalary']] = scaler.fit_transform(df[['CreditScore','Balance','EstimatedSalary']])

"""## Coorelation Matrix Heatmap"""

plt.figure(figsize=(12,12))
sns.heatmap(df.corr(),annot=True,cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""# Train Test Split"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(df.drop('Churn',axis=1),df['Churn'],test_size=0.3,random_state=42)

"""# Churn Prediction

## Decision Tree
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
#creating Decision Tree Classifer object
dtree = DecisionTreeClassifier()

#defining parameter range
param_grid = {
    'max_depth': [2,4,6,8,10,12,14,16,18,20],
    'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],
    'criterion': ['gini', 'entropy'],
    'random_state': [0,42]
    }

#Creating grid search object
grid_dtree = GridSearchCV(dtree, param_grid, cv = 5, scoring = 'roc_auc', n_jobs = -1, verbose = 1)

#Fitting the grid search object to the training data
grid_dtree.fit(X_train, y_train)

#Printing the best parameters
print('Best parameters found: ', grid_dtree.best_params_)

dtree = DecisionTreeClassifier(criterion='gini', max_depth=6, random_state=42, min_samples_leaf=10)
dtree

#training the model
dtree.fit(X_train,y_train)
#training accuracy
dtree.score(X_train,y_train)

dtree_pred = dtree.predict(X_test)

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier
#creating Random Forest Classifer object
rfc = RandomForestClassifier()

#defining parameter range
param_grid = {
    'max_depth': [2,4,6,8,10],
    'min_samples_leaf': [2,4,6,8,10],
    'criterion': ['gini', 'entropy'],
    'random_state': [0,42]
    }

#Creating grid search object
grid_rfc = GridSearchCV(rfc, param_grid, cv = 5, scoring = 'roc_auc', n_jobs = -1, verbose = 1)

#Fitting the grid search object to the training data
grid_rfc.fit(X_train, y_train)

#Printing the best parameters
print('Best parameters found: ', grid_rfc.best_params_)

rfc = RandomForestClassifier(min_samples_leaf=8, max_depth=10, random_state=0, criterion='entropy')
rfc

#training the model
rfc.fit(X_train, y_train)
#model accuracy
rfc.score(X_train, y_train)

rfc_pred = rfc.predict(X_test)

"""# Model Evaluation

## Decison Tree

Confusion Matroks Headmap
"""

from sklearn.metrics import confusion_matrix
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test,dtree_pred),annot=True,fmt='d',cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Decision Tree')
plt.show()

"""The True Positive shows the count of correctly classified data points whereas the False Positive elements are those that are misclassified by the model. The higher the True Positive values of the confusion matrix the better, indicating many correct predictions."""

ax = sns.distplot(y_test, hist=False, color="r", label="Actual Value")
sns.distplot(dtree_pred, hist=False, color="b", label="Fitted Values" , ax=ax)

"""Classification Report"""

from sklearn.metrics import classification_report
print(classification_report(y_test, dtree_pred))

from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score
print("Accuracy Score: ", accuracy_score(y_test, dtree_pred))
print("Mean Absolute Error: ", mean_absolute_error(y_test, dtree_pred))
print("R2 Score: ", r2_score(y_test, dtree_pred))

"""## Random Forest Classifier

Confusion Matrix Heatmap
"""

plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_test,rfc_pred),annot=True,fmt='d',cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Random Forest Classifier')
plt.show()

"""The True Positive shows the count of correctly classified data points whereas the False Positive elements are those that are misclassified by the model. The higher the True Positive values of the confusion matrix the better, indicating many correct predictions.

## Distribution Plot
"""

ax = sns.distplot(y_test, hist=False, color="r", label="Actual Value")
sns.distplot(rfc_pred, hist=False, color="b", label="Fitted Values" , ax=ax)

from sklearn.metrics import classification_report
print(classification_report(y_test, rfc_pred))

print("Accuracy Score: ", accuracy_score(y_test, rfc_pred))
print("Mean Absolute Error: ", mean_absolute_error(y_test, rfc_pred))
print("R2 Score: ", r2_score(y_test, rfc_pred))

"""# Conclution

From the exploratory data analysis, I have concluded that the churn count of the customersdepends upon the following factors:

1. Age
2. Geography
3. Tenure
4. Balance
5. Number of Products
6. Has Credit Card
7. Is Active Member

Coming to the classification models, I have used the following models:

1. Decision Tree Classifier
2. Random Forest Classifier

Based on the two methods that have been tested, it was found that radom forest accuracy produced the greatest accuracy with a percentage of 87%. whereas the Decision Tree only produces an accuracy percentage of 86%.
"""